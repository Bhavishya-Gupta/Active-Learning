{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL ASSIGNMENT OF ACTIVE LEARNING PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY 1: USING NEURAL NETWORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/10, Loss: 1.7455806453585625\n",
      "Epoch 2/10, Loss: 1.3378883168798685\n",
      "Epoch 3/10, Loss: 1.192586838748604\n",
      "Epoch 4/10, Loss: 1.1020615001526475\n",
      "Epoch 5/10, Loss: 1.0350671640495956\n",
      "Epoch 6/10, Loss: 0.979087567339465\n",
      "Epoch 7/10, Loss: 0.9408895053713768\n",
      "Epoch 8/10, Loss: 0.8955124899016507\n",
      "Epoch 9/10, Loss: 0.8617812046063319\n",
      "Epoch 10/10, Loss: 0.8321819068693672\n",
      "Accuracy on the test set: 62.14%\n",
      "Average Least Confidence: 0.2873304784297943\n",
      "Average Prediction Entropy: 0.8017570376396179\n",
      "Average Margin Sampling: 0.28619784116744995\n",
      "Average Cosine Similarity: 0.45348525047302246\n",
      "Average L2 Norm: 0.9036170840263367\n",
      "Average KL Divergence: 0.4173566401004791\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "# This includes the essential PyTorch and torchvision modules for working with neural networks and datasets.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Download and load the CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# Define the classes for classification\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class (nn.Module) to initialize the base class\n",
    "        super(Net, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)  # Input channels: 3, Output channels: 6, Kernel size: 5x5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  # Input channels: 6, Output channels: 16, Kernel size: 5x5\n",
    "        # Fully connected layers\n",
    "        # Flatten layer to convert 3D tensor to 1D tensor before fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # Input features: 16*5*5, Output features: 120\n",
    "        self.fc2 = nn.Linear(120, 84)  # Input features: 120, Output features: 84\n",
    "        self.fc3 = nn.Linear(84, 10)  # Input features: 84, Output features: 10 (number of classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2, 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2, 2)\n",
    "        # Flatten the output to a 1D tensor before passing it to fully connected layers\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        # First fully connected layer (fc1) followed by ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Second fully connected layer (fc2) followed by ReLU activation\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Final fully connected layer (fc3) for classification output\n",
    "        x = self.fc3(x)\n",
    "        # Return the final output \n",
    "        return x\n",
    "\n",
    "# Instantiate the network\n",
    "net = Net()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # SGD optimizer with momentum\n",
    "epochs = 10  # Number of times to iterate through the entire dataset during training\n",
    "\n",
    "# Creating a function to calculate uncertainty for each epoch metrics\n",
    "def calculate_uncertainty_metrics(outputs):\n",
    "    # Convert NumPy array to PyTorch tensor\n",
    "    outputs_tensor = torch.from_numpy(outputs)\n",
    "    # Applying softmax along dimension 1\n",
    "    probabilities = F.softmax(outputs_tensor, dim=1)\n",
    "    # Least Confidence: 1 - Maximum probability for each sample\n",
    "    least_confidence = 1 - probabilities.max(dim=1).values.cpu().detach().numpy()\n",
    "    # Handling NaN in prediction entropy\n",
    "    current_probs = probabilities.clone().detach()\n",
    "    current_probs[current_probs == 0] = 1e-10  # Adding a small epsilon to avoid log(0)\n",
    "    # Prediction Entropy: Negative sum of (probability * log(probability)) for each class\n",
    "    prediction_entropy = -torch.sum(current_probs * torch.log(current_probs), dim=1).cpu().detach().numpy()\n",
    "    # Margin Sampling: 1 - (Maximum probability - Minimum probability) for each sample\n",
    "    margin_sampling = 1 - torch.max(probabilities, dim=1).values.cpu().detach().numpy() - \\\n",
    "                      torch.min(probabilities, dim=1).values.cpu().detach().numpy()\n",
    "    # Returning least confidence,prediction entropy and margin sampling values obtained\n",
    "    return least_confidence, prediction_entropy, margin_sampling\n",
    "\n",
    "# Creating a function to caalculate diversiy metrics\n",
    "def calculate_diversity_metrics(features, m=5):\n",
    "    # Calculate pairwise distances using cosine similarity\n",
    "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
    "    # Cosine Similarity: 1 - Mean cosine similarity with the top m neighbors for each sample\n",
    "    cosine_similarity = 1 - feature_distances[:, 1:m+1].mean(axis=1)\n",
    "    # Calculate pairwise distances using L2 (Euclidean) norm\n",
    "    l2_distances = pairwise_distances(features.cpu().detach().numpy(), metric='euclidean')\n",
    "    # L2 Norm: Mean L2 norm with the top m neighbors for each sample\n",
    "    l2_norm = l2_distances[:, 1:m+1].mean(axis=1)\n",
    "    # Returning cosine similarity and l2 norm values obtained \n",
    "    return cosine_similarity, l2_norm\n",
    "\n",
    "# Creating a functiom to calculate kl divergence \n",
    "def calculate_kl_divergence(outputs, feature_distances, m=5):\n",
    "    # List to store KL divergence scores for each sample\n",
    "    kl_divergence = []\n",
    "    # Iterate over each sample in the outputs\n",
    "    for i in range(len(outputs)):\n",
    "        # Calculate the probability distribution of the current sample\n",
    "        current_sample_prob = F.softmax(outputs[i], dim=0)\n",
    "        # Get the indices of the top m neighbors for the current sample\n",
    "        neighbor_indices = feature_distances[i, 1:m+1].astype(int)\n",
    "        # Calculate the average probability distribution of the neighbors\n",
    "        neighbors_prob = torch.mean(F.softmax(outputs[neighbor_indices], dim=1), dim=0)\n",
    "        # Calculate KL divergence between the current sample and its neighbors\n",
    "        # Include 'reduction' argument inside F.kl_div\n",
    "        kl_divergence.append(F.kl_div(torch.log(current_sample_prob), neighbors_prob, reduction='batchmean'))\n",
    "    \n",
    "    # Returning kl divergence values obtained\n",
    "    return kl_divergence\n",
    "\n",
    "# Creating a Function to calculate uncertainty and diversity metrics\n",
    "def calculate_metrics(outputs, features, m=5):\n",
    "    # Calculate uncertainty metrics\n",
    "    least_confidence, prediction_entropy, margin_sampling = calculate_uncertainty_metrics(outputs.detach().numpy())\n",
    "    # Extend lists with uncertainty metrics\n",
    "    least_confidence_list.extend(torch.from_numpy(least_confidence))\n",
    "    prediction_entropy_list.extend(torch.from_numpy(prediction_entropy))\n",
    "    margin_sampling_list.extend(torch.from_numpy(margin_sampling))\n",
    "    # Calculate diversity metrics\n",
    "    features_normalized = F.normalize(features, p=2, dim=1)\n",
    "    cosine_similarity, l2_norm = calculate_diversity_metrics(features_normalized)\n",
    "    # cosine_similarity, l2_norm = calculate_diversity_metrics(features)\n",
    "    # Extend lists with diversity metrics\n",
    "    cosine_similarity_list.extend(torch.from_numpy(cosine_similarity))\n",
    "    l2_norm_list.extend(torch.from_numpy(l2_norm))\n",
    "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
    "    # Calculate KL divergence scores\n",
    "    kl_divergence_scores = calculate_kl_divergence(outputs, feature_distances, m=5)\n",
    "    # Extend the list with KL divergence scores\n",
    "    kl_divergence_list.extend(kl_divergence_scores)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):  # Loop over the dataset for a specified number of epochs\n",
    "    running_loss = 0.0  # Initialize the running loss for the current epoch\n",
    "    for i, data in enumerate(trainloader, 0):  # Iterate over batches in the training loader\n",
    "        inputs, labels = data  # Get inputs and labels for the current batch\n",
    "        optimizer.zero_grad()  # Zero the gradients to clear previous gradients\n",
    "        outputs = net(inputs)  # Forward pass to compute the predicted outputs\n",
    "        loss = criterion(outputs, labels)  # Compute the loss between predicted and true labels\n",
    "        loss.backward()  # Backward pass to compute gradients of the loss with respect to model parameters\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "        running_loss += loss.item()  # Accumulate the running loss for statistics\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(trainloader)}\")\n",
    "\n",
    "# Testing the model\n",
    "correct = 0  # Initialize the number of correctly predicted samples\n",
    "total = 0  # Initialize the total number of samples\n",
    "\n",
    "# Initialize lists to store uncertainty and diversity measures\n",
    "least_confidence_list = [] # Initialize lists to store least confidence measures\n",
    "prediction_entropy_list = [] # Initialize lists to store prediction entropy measures\n",
    "margin_sampling_list = [] # Initialize lists to store margin sampling measures\n",
    "cosine_similarity_list = [] # Initialize lists to store cosine similarity measures\n",
    "l2_norm_list = [] # Initialize lists to store L2 norm estimates\n",
    "kl_divergence_list = [] # Initialize lists to store KL divergence estimates\n",
    "\n",
    "# Use torch.no_grad() to disable gradient computation during testing\n",
    "with torch.no_grad():\n",
    "    for data in testloader:  # Iterate over batches in the test loader\n",
    "        images, labels = data  # Get inputs and true labels for the current batch\n",
    "        outputs = net(images)  # Forward pass to compute predicted outputs\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get the index of the maximum predicted value\n",
    "        total += labels.size(0)  # Increment the total number of samples by the batch size\n",
    "        correct += (predicted == labels).sum().item()  # Count the number of correctly predicted samples\n",
    "\n",
    "        # Extract features using the first convolutional layer\n",
    "        features = net.conv1(images)\n",
    "        # Apply max pooling and ReLU activation\n",
    "        features = F.max_pool2d(F.relu(features), 2, 2)\n",
    "        # Extract features using the second convolutional layer\n",
    "        features = net.conv2(features)\n",
    "        # Apply max pooling and ReLU activation\n",
    "        features = F.max_pool2d(F.relu(features), 2, 2)\n",
    "        # Flatten the features to be used in fully connected layers\n",
    "        features = features.view(features.size(0), -1)\n",
    "        # Calculate metrics using the extracted features and model outputs\n",
    "        calculate_metrics(outputs, features)\n",
    "\n",
    "accuracy = 100 * correct / total # Calculate accuracy\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}%\") # Print the accuracy on test set \n",
    "# Print the average values of uncertainty and diversity measures\n",
    "print(f\"Average Least Confidence: {torch.mean(torch.stack(least_confidence_list))}\") # Print the average value of least confidence\n",
    "print(f\"Average Prediction Entropy: {torch.mean(torch.stack(prediction_entropy_list))}\") # Print the average value of prediction entropy\n",
    "print(f\"Average Margin Sampling: {torch.mean(torch.stack(margin_sampling_list))}\") # Print the average value of margin sampling\n",
    "print(f\"Average Cosine Similarity: {torch.mean(torch.stack(cosine_similarity_list))}\") # Print the average value of cosine similarity\n",
    "print(f\"Average L2 Norm: {torch.mean(torch.stack(l2_norm_list))}\") # Print the average value of L2 norm\n",
    "print(f\"Average KL Divergence: {torch.mean(torch.stack(kl_divergence_list))}\") # Print the average value of KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY 2: USING CNN ARCHITECTURE AND DOING MODIFACTIONS FOR BETTER ACCURACY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/10, Loss: 0.8350726962089539\n",
      "Epoch 2/10, Loss: 0.4385553002357483\n",
      "Epoch 3/10, Loss: 0.3632165491580963\n",
      "Epoch 4/10, Loss: 0.7260843515396118\n",
      "Epoch 5/10, Loss: 0.5566812753677368\n",
      "Epoch 6/10, Loss: 0.19536720216274261\n",
      "Epoch 7/10, Loss: 0.16471228003501892\n",
      "Epoch 8/10, Loss: 0.23137861490249634\n",
      "Epoch 9/10, Loss: 0.06536935269832611\n",
      "Epoch 10/10, Loss: 0.030354849994182587\n",
      "Test Accuracy: 72.18%\n",
      "Average Least Confidence: 0.08586908876895905\n",
      "Average Prediction Entropy: 0.22230054438114166\n",
      "Average Margin Sampling: 0.08586746454238892\n",
      "Average Cosine Similarity: 0.5769681930541992\n",
      "Average L2 Norm: 0.9092544913291931\n",
      "Average KL Divergence: 1.3938003778457642\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "# This includes the essential PyTorch and torchvision modules for working with neural networks and datasets.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.stats import entropy\n",
    "# Define the CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "    \n",
    "        # Convolutional layer 1: Input channels=3, output channels=32, kernel size=3, padding=1\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        # Rectified Linear Unit (ReLU) activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # Max pooling layer 1: Kernel size=2, stride=2\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        # Convolutional layer 2: Input channels=32, output channels=64, kernel size=3, padding=1\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        # Flatten layer to convert 3D tensor to 1D tensor\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Fully connected layer 1: Input features=64*8*8, output features=512\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        # Fully connected layer 2: Input features=512, output features=10 (output classes)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Convolutional layer 1: Input tensor x undergoes convolution\n",
    "        x = self.conv1(x)\n",
    "        # Apply ReLU activation function to introduce non-linearity\n",
    "        x = self.relu(x)\n",
    "        # Perform max pooling to down-sample the spatial dimensions\n",
    "        x = self.maxpool(x)\n",
    "    \n",
    "        # Convolutional layer 2: Apply another convolution operation\n",
    "        x = self.conv2(x)\n",
    "        # Apply ReLU activation\n",
    "        x = self.relu(x)\n",
    "        # Another max pooling operation\n",
    "        x = self.maxpool(x)\n",
    "    \n",
    "        # Flatten the tensor to prepare for fully connected layers\n",
    "        x = self.flatten(x)\n",
    "    \n",
    "        # Fully connected layer 1: Apply linear transformation\n",
    "        x = self.fc1(x)\n",
    "        # Apply ReLU activation\n",
    "        x = self.relu(x)\n",
    "    \n",
    "        # Fully connected layer 2: Produce the final output\n",
    "        x = self.fc2(x)\n",
    "    \n",
    "        # Return the final output tensor after passing through the network\n",
    "        return x\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Hyperparameters\n",
    "batch_size = 64 # Set the batch size for training\n",
    "learning_rate = 0.001 # Set the learning rate for the optimizer\n",
    "epochs = 10 # Set the number of training epochs\n",
    "m = 5  # Number of nearest neighbors for diversity measures\n",
    "\n",
    "# Define data transformation for CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize pixel values\n",
    "])\n",
    "\n",
    "# Download and Load CIFAR-10 training and test datasets\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)  # Create training dataset\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)  # Create test dataset\n",
    "\n",
    "# Create DataLoader instances for training and test datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the neural network model, loss function, and optimizer\n",
    "model = SimpleCNN().to(device)  # Move the model to GPU if available\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer\n",
    "\n",
    "# Creating a function to calculate uncertainty for each epoch metrics\n",
    "def calculate_uncertainty_metrics(outputs):\n",
    "    # Convert NumPy array to PyTorch tensor\n",
    "    outputs_tensor = torch.from_numpy(outputs)\n",
    "    # Applying softmax along dimension 1\n",
    "    probabilities = F.softmax(outputs_tensor, dim=1)\n",
    "    # Least Confidence: 1 - Maximum probability for each sample\n",
    "    least_confidence = 1 - probabilities.max(dim=1).values.cpu().detach().numpy()\n",
    "    # Handling NaN in prediction entropy\n",
    "    current_probs = probabilities.clone().detach()\n",
    "    current_probs[current_probs == 0] = 1e-10  # Adding a small epsilon to avoid log(0)\n",
    "    # Prediction Entropy: Negative sum of (probability * log(probability)) for each class\n",
    "    prediction_entropy = -torch.sum(current_probs * torch.log(current_probs), dim=1).cpu().detach().numpy()\n",
    "    # Margin Sampling: 1 - (Maximum probability - Minimum probability) for each sample\n",
    "    margin_sampling = 1 - torch.max(probabilities, dim=1).values.cpu().detach().numpy() - \\\n",
    "                      torch.min(probabilities, dim=1).values.cpu().detach().numpy()\n",
    "    # Returning least confidence,prediction entropy and margin sampling values obtained\n",
    "    return least_confidence, prediction_entropy, margin_sampling\n",
    "\n",
    "# Creating a function to caalculate diversiy metrics\n",
    "def calculate_diversity_metrics(features, m=5):\n",
    "    # Calculate pairwise distances using cosine similarity\n",
    "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
    "    # Cosine Similarity: 1 - Mean cosine similarity with the top m neighbors for each sample\n",
    "    cosine_similarity = 1 - feature_distances[:, 1:m+1].mean(axis=1)\n",
    "    # Calculate pairwise distances using L2 (Euclidean) norm\n",
    "    l2_distances = pairwise_distances(features.cpu().detach().numpy(), metric='euclidean')\n",
    "    # L2 Norm: Mean L2 norm with the top m neighbors for each sample\n",
    "    l2_norm = l2_distances[:, 1:m+1].mean(axis=1)\n",
    "    # Returning cosine similarity and l2 norm values obtained \n",
    "    return cosine_similarity, l2_norm\n",
    "\n",
    "\n",
    "# Creating a function to calculate kl divergence \n",
    "def calculate_kl_divergence(outputs, feature_distances, m=5):\n",
    "    # List to store KL divergence scores for each sample\n",
    "    kl_divergence = []\n",
    "    # Iterate over each sample in the outputs\n",
    "    for i in range(len(outputs)):\n",
    "        # Calculate the probability distribution of the current sample\n",
    "        current_sample_prob = F.softmax(outputs[i], dim=0)\n",
    "        # Get the indices of the top m neighbors for the current sample\n",
    "        neighbor_indices = feature_distances[i, 1:m+1].astype(int)\n",
    "        # Calculate the average probability distribution of the neighbors\n",
    "        neighbors_prob = torch.mean(F.softmax(outputs[neighbor_indices], dim=1), dim=0)\n",
    "\n",
    "        # Add a small epsilon to avoid zero probabilities\n",
    "        epsilon = 1e-10\n",
    "        current_sample_prob = current_sample_prob + epsilon\n",
    "        neighbors_prob = neighbors_prob + epsilon\n",
    "\n",
    "        # Calculate KL divergence between the current sample and its neighbors\n",
    "        kl_divergence.append(F.kl_div(torch.log(current_sample_prob), neighbors_prob, reduction='batchmean'))\n",
    "    \n",
    "    # Returning kl divergence values obtained\n",
    "    return kl_divergence\n",
    "\n",
    "\n",
    "# Creating a Function to calculate uncertainty and diversity metrics\n",
    "def calculate_metrics(outputs, features, m=5):\n",
    "    # Calculate uncertainty metrics\n",
    "    least_confidence, prediction_entropy, margin_sampling = calculate_uncertainty_metrics(outputs.detach().numpy())\n",
    "    # Extend lists with uncertainty metrics\n",
    "    least_confidence_list.extend(torch.from_numpy(least_confidence))\n",
    "    prediction_entropy_list.extend(torch.from_numpy(prediction_entropy))\n",
    "    margin_sampling_list.extend(torch.from_numpy(margin_sampling))\n",
    "    # Calculate diversity metrics\n",
    "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
    "    features_normalized = F.normalize(features, p=2, dim=1)\n",
    "    cosine_similarity, l2_norm = calculate_diversity_metrics(features_normalized)\n",
    "    # cosine_similarity, l2_norm = calculate_diversity_metrics(features)\n",
    "    # Extend lists with diversity metrics\n",
    "    cosine_similarity_list.extend(torch.from_numpy(cosine_similarity))\n",
    "    # Inside the calculate_metrics function\n",
    "    l2_norm_list.extend(torch.from_numpy(l2_norm))\n",
    "    # Calculate KL divergence scores\n",
    "    kl_divergence_scores = calculate_kl_divergence(outputs, feature_distances, m=5)\n",
    "    # Extend the list with KL divergence scores\n",
    "    kl_divergence_list.extend(kl_divergence_scores)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs): # Iterate over each epoch\n",
    "    model.train()  # Set the model to training mode\n",
    "    for images, labels in train_loader: # Iterate through batches of training data\n",
    "        images, labels = images.to(device), labels.to(device) # Move data to GPU if available\n",
    "        optimizer.zero_grad()  # Zero gradients to clear previous gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass to compute gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")  # Print the loss for the current epoch\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval() # Set the model to evaluation mode\n",
    "correct = 0  # Initialize the number of correctly predicted samples\n",
    "total = 0  # Initialize the total number of samples\n",
    "\n",
    "least_confidence_list= [] # Initialize an empty list to store least confidence values\n",
    "prediction_entropy_list = [] # Initialize an empty list to store prediction entropy values\n",
    "margin_sampling_list = [] # Initialize an empty list to store margin sampling values\n",
    "cosine_similarity_list = [] # Initialize an empty list to store cosine similarity values\n",
    "l2_norm_list = [] # Initialize an empty list to store L2 norm values\n",
    "kl_divergence_list = [] # Initialize an empty list to store KL Divergence values\n",
    "\n",
    "with torch.no_grad(): # Use torch.no_grad() to disable gradient computation during testing\n",
    "    for images, labels in test_loader: # Iterate through batches of test data\n",
    "        images, labels = images.to(device), labels.to(device) # Move data to GPU if available\n",
    "        outputs = model(images) # Forward pass through the model\n",
    "        _, predicted = torch.max(outputs.data, 1) # Get the predicted class labels\n",
    "        total += labels.size(0) # Increment total number of images\n",
    "        correct += (predicted == labels).sum().item() # Count correctly predicted images\n",
    "\n",
    "        # Apply convolutional layer 1 to the input images\n",
    "        conv1_output = model.conv1(images)\n",
    "        # Apply ReLU activation function\n",
    "        relu_output = model.relu(conv1_output)\n",
    "        # Apply max pooling\n",
    "        maxpool_output = model.maxpool(relu_output)\n",
    "        # Apply convolutional layer 2 to the max-pooled output\n",
    "        conv2_output = model.conv2(maxpool_output)\n",
    "        # Flatten the output\n",
    "        features = model.flatten(conv2_output)\n",
    "        # Reshape the features to have a consistent size\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        # Calculate metrics using the extracted features and model outputs\n",
    "        calculate_metrics(outputs, features)\n",
    "        \n",
    "# Calculate accuracy\n",
    "accuracy = correct / total # Calculate accuracy\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\") # Print the accuracy on the test seT\n",
    "\n",
    "# Print the average values of uncertainty and diversity measures\n",
    "print(f\"Average Least Confidence: {torch.mean(torch.stack(least_confidence_list))}\") # Print the average value of least confidence\n",
    "print(f\"Average Prediction Entropy: {torch.mean(torch.stack(prediction_entropy_list))}\") # Print the average value of prediction entropy\n",
    "print(f\"Average Margin Sampling: {torch.mean(torch.stack(margin_sampling_list))}\") # Print the average value of margin sampling\n",
    "print(f\"Average Cosine Similarity: {torch.mean(torch.stack(cosine_similarity_list))}\") # Print the average value of cosine similarity\n",
    "print(f\"Average L2 Norm: {torch.mean(torch.stack(l2_norm_list))}\") # Print the average value of L2 norm\n",
    "print(f\"Average KL Divergence: {torch.mean(torch.stack(kl_divergence_list))}\") # Print the average value of KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY 3: USING PRETRAINED MODEL: RESNET 18 FOR ACHIEVING BETTER ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1, Loss: 1.3471097491128976\n",
      "Epoch 2, Loss: 1.062493772610374\n",
      "Epoch 3, Loss: 0.9705260539298777\n",
      "Epoch 4, Loss: 0.919052358935861\n",
      "Epoch 5, Loss: 0.8840376783133773\n",
      "Epoch 6, Loss: 0.8539965738497122\n",
      "Epoch 7, Loss: 0.8247907693733645\n",
      "Epoch 8, Loss: 0.806672897675763\n",
      "Epoch 9, Loss: 0.7819206634979419\n",
      "Epoch 10, Loss: 0.76822626811769\n",
      "Accuracy on the test set: 73.00%\n",
      "Average Least Confidence: 0.2607212960720062\n",
      "Average Prediction Entropy: 0.7505708336830139\n",
      "Average Margin Sampling: 0.25922003388404846\n",
      "Average Cosine Similarity: 0.8664329051971436\n",
      "Average L2 Norm: 0.5039328336715698\n",
      "Average KL Divergence: 0.48649540543556213\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "# This includes the essential PyTorch and torchvision modules for working with neural networks and datasets.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Define the transform for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.RandomResizedCrop(32),    # Randomly crop the image and resize to 32x32\n",
    "    transforms.ToTensor(),               # Convert image to PyTorch tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image pixel values\n",
    "])\n",
    "\n",
    "# Download and load the CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) # Create training dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) # Create test dataset\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Use a pre-trained ResNet18 model\n",
    "# model = models.resnet18(pretrained=True)\n",
    "model = models.resnet18(weights='IMAGENET1K_V1') # Initialize the ResNet18 model with pre-trained weights from the ImageNet dataset\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)  # Change the output layer to have 10 classes\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Check if GPU is available, else use CPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # SGD optimizer with momentum\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10 # Set the number of training epochs\n",
    "m = 5  # Number of nearest neighbors for diversity measures\n",
    "\n",
    "# Creating a function to calculate uncertainty for each epoch metrics\n",
    "def calculate_uncertainty_metrics(outputs):\n",
    "    # Convert NumPy array to PyTorch tensor\n",
    "    outputs_tensor = torch.from_numpy(outputs)\n",
    "    # Applying softmax along dimension 1\n",
    "    probabilities = F.softmax(outputs_tensor, dim=1)\n",
    "    # Least Confidence: 1 - Maximum probability for each sample\n",
    "    least_confidence = 1 - probabilities.max(dim=1).values.cpu().detach().numpy()\n",
    "    # Handling NaN in prediction entropy\n",
    "    current_probs = probabilities.clone().detach()\n",
    "    current_probs[current_probs == 0] = 1e-10  # Adding a small epsilon to avoid log(0)\n",
    "    # Prediction Entropy: Negative sum of (probability * log(probability)) for each class\n",
    "    prediction_entropy = -torch.sum(current_probs * torch.log(current_probs), dim=1).cpu().detach().numpy()\n",
    "    # Margin Sampling: 1 - (Maximum probability - Minimum probability) for each sample\n",
    "    margin_sampling = 1 - torch.max(probabilities, dim=1).values.cpu().detach().numpy() - \\\n",
    "                      torch.min(probabilities, dim=1).values.cpu().detach().numpy()\n",
    "    # Returning least confidence,prediction entropy and margin sampling values obtained\n",
    "    return least_confidence, prediction_entropy, margin_sampling\n",
    "\n",
    "# Creating a function to caalculate diversiy metrics\n",
    "def calculate_diversity_metrics(features, m=5):\n",
    "    # Calculate pairwise distances using cosine similarity\n",
    "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
    "    # Cosine Similarity: 1 - Mean cosine similarity with the top m neighbors for each sample\n",
    "    cosine_similarity = 1 - feature_distances[:, 1:m+1].mean(axis=1)\n",
    "    # Calculate pairwise distances using L2 (Euclidean) norm\n",
    "    l2_distances = pairwise_distances(features.cpu().detach().numpy(), metric='euclidean')\n",
    "    # L2 Norm: Mean L2 norm with the top m neighbors for each sample\n",
    "    l2_norm = l2_distances[:, 1:m+1].mean(axis=1)\n",
    "    # Returning cosine similarity and l2 norm values obtained \n",
    "    return cosine_similarity, l2_norm\n",
    "\n",
    "# Creating a function to calculate kl divergence \n",
    "def calculate_kl_divergence(outputs, feature_distances, m=5):\n",
    "    # List to store KL divergence scores for each sample\n",
    "    kl_divergence = []\n",
    "    # Iterate over each sample in the outputs\n",
    "    for i in range(len(outputs)):\n",
    "        # Calculate the probability distribution of the current sample\n",
    "        current_sample_prob = F.softmax(outputs[i], dim=0)\n",
    "        # Get the indices of the top m neighbors for the current sample\n",
    "        neighbor_indices = feature_distances[i, 1:m+1].astype(int)\n",
    "        # Calculate the average probability distribution of the neighbors\n",
    "        neighbors_prob = torch.mean(F.softmax(outputs[neighbor_indices], dim=1), dim=0)\n",
    "\n",
    "        # Add a small epsilon to avoid zero probabilities\n",
    "        epsilon = 1e-10\n",
    "        current_sample_prob = current_sample_prob + epsilon\n",
    "        neighbors_prob = neighbors_prob + epsilon\n",
    "\n",
    "        # Calculate KL divergence between the current sample and its neighbors\n",
    "        kl_divergence.append(F.kl_div(torch.log(current_sample_prob), neighbors_prob, reduction='batchmean'))\n",
    "    \n",
    "    # Returning kl divergence values obtained\n",
    "    return kl_divergence\n",
    "\n",
    "# Creating a Function to calculate uncertainty and diversity metrics\n",
    "def calculate_metrics(outputs, features, m=5):\n",
    "    # Calculate uncertainty metrics\n",
    "    least_confidence, prediction_entropy, margin_sampling = calculate_uncertainty_metrics(outputs.detach().numpy())\n",
    "    # Extend lists with uncertainty metrics\n",
    "    least_confidence_list.extend(torch.from_numpy(least_confidence))\n",
    "    prediction_entropy_list.extend(torch.from_numpy(prediction_entropy))\n",
    "    margin_sampling_list.extend(torch.from_numpy(margin_sampling))\n",
    "    # Calculate diversity metrics\n",
    "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
    "    features_normalized = F.normalize(features, p=2, dim=1)\n",
    "    cosine_similarity, l2_norm = calculate_diversity_metrics(features_normalized)\n",
    "    # cosine_similarity, l2_norm = calculate_diversity_metrics(features)\n",
    "    # Extend lists with diversity metrics\n",
    "    cosine_similarity_list.extend(torch.from_numpy(cosine_similarity))\n",
    "    # Inside the calculate_metrics function\n",
    "    l2_norm_list.extend(torch.from_numpy(l2_norm))\n",
    "    # Calculate KL divergence scores\n",
    "    kl_divergence_scores = calculate_kl_divergence(outputs, feature_distances, m=5)\n",
    "    # Extend the list with KL divergence scores\n",
    "    kl_divergence_list.extend(kl_divergence_scores)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0  # Variable to track the running loss during each epoch\n",
    "    # Iterate over the training dataset\n",
    "    for i, data in enumerate(trainloader, 0): \n",
    "        inputs, labels = data # Get inputs and labels for the current batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate the loss\n",
    "        loss.backward()  # Backward pass to compute gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "        running_loss += loss.item()  # Accumulate the running loss\n",
    "\n",
    "    # Print the average loss for the current epoch\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}\")\n",
    "\n",
    "\n",
    "# Testing the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0  # Variable to track the number of correctly predicted images\n",
    "total = 0  # Variable to track the total number of images in the test set\n",
    "\n",
    "least_confidence_list = [] # Initialize an empty list to store least confidence values\n",
    "prediction_entropy_list = [] # Initialize an empty list to store prediction entropy values\n",
    "margin_sampling_list = [] # Initialize an empty list to store margin sampling values\n",
    "cosine_similarity_list = [] # Initialize an empty list to store cosine similarity values\n",
    "l2_norm_list = [] # Initialize an empty list to store L2 norm values\n",
    "kl_divergence_list = [] # Initialize an empty list to store KL Divergence values\n",
    "\n",
    "with torch.no_grad(): # Use torch.no_grad() to disable gradient computation during testing\n",
    "    for data in testloader:  # Iterate over batches in the test loader\n",
    "        images, labels = data # Get inputs and labels for the current batch\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU if available\n",
    "        outputs = model(images)  # Forward pass\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predicted class labels\n",
    "        total += labels.size(0)  # Increment total number of images\n",
    "        correct += (predicted == labels).sum().item()  # Count correctly predicted images\n",
    "\n",
    "        # Diversity measures\n",
    "\n",
    "        # Extract features from the model layers\n",
    "        features = model.conv1(images)  # Apply convolutional layer 1 to the input images\n",
    "        features = model.relu(features) # Apply Rectified Linear Unit (ReLU) activation function to introduce non-linearity\n",
    "        features = model.maxpool(features) # Apply max pooling to reduce spatial dimensions and retain important information\n",
    "        features = model.layer1(features)  # Use layer1 instead of conv2 for ResNet\n",
    "        features = model.layer2(features)  # Use layer2 instead of conv2 for ResNet\n",
    "        features = model.avgpool(features)  # Use avgpool instead of maxpool for ResNet\n",
    "\n",
    "        # Flatten the features for distance calculations\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        calculate_metrics(outputs, features)\n",
    "\n",
    "accuracy = 100 * correct / total # Calculate accuracy\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}%\") # Print the test accuracy\n",
    "\n",
    "# Print the average values of uncertainty and diversity measures\n",
    "print(f\"Average Least Confidence: {torch.mean(torch.stack(least_confidence_list))}\") # Print the average value of least confidence\n",
    "print(f\"Average Prediction Entropy: {torch.mean(torch.stack(prediction_entropy_list))}\") # Print the average value of prediction entropy\n",
    "print(f\"Average Margin Sampling: {torch.mean(torch.stack(margin_sampling_list))}\") # Print the average value of margin sampling\n",
    "print(f\"Average Cosine Similarity: {torch.mean(torch.stack(cosine_similarity_list))}\") # Print the average value of cosine similarity\n",
    "print(f\"Average L2 Norm: {torch.mean(torch.stack(l2_norm_list))}\") # Print the average value of L2 norm\n",
    "print(f\"Average KL Divergence: {torch.mean(torch.stack(kl_divergence_list))}\") # Print the average value of KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY 4: USING ANOTHER PRETRAINED MODEL: DENSENET 121 MODEL FOR BETTER ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/10, Loss: 0.6909970045089722\n",
      "Epoch 2/10, Loss: 1.0706369876861572\n",
      "Epoch 3/10, Loss: 0.46002820134162903\n",
      "Epoch 4/10, Loss: 0.2053932100534439\n",
      "Epoch 5/10, Loss: 0.19087783992290497\n",
      "Epoch 6/10, Loss: 0.1159883439540863\n",
      "Epoch 7/10, Loss: 0.38801509141921997\n",
      "Epoch 8/10, Loss: 0.23865385353565216\n",
      "Epoch 9/10, Loss: 0.1268896460533142\n",
      "Epoch 10/10, Loss: 0.48889589309692383\n",
      "Test Accuracy: 84.22%\n",
      "Average Least Confidence: 0.08035318553447723\n",
      "Average Prediction Entropy: 0.22045551240444183\n",
      "Average Margin Sampling: 0.08034031093120575\n",
      "Average Cosine Similarity: 0.13549207150936127\n",
      "Average L2 Norm: 1.2808501720428467\n",
      "Average KL Divergence: 1.0776267051696777\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries \n",
    "# This includes the essential PyTorch and torchvision modules for working with neural networks and datasets.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Step 1: Set device and hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check if GPU is available, else use CPU\n",
    "batch_size = 64  # Number of images in each mini-batch\n",
    "learning_rate = 0.001  # Learning rate for the optimizer\n",
    "epochs = 10 # Number of times to iterate through the entire dataset during training\n",
    "m = 5  # Number of nearest neighbors for diversity measures \n",
    "\n",
    "# Step 2: Load and preprocess the CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image pixel values\n",
    "])\n",
    "\n",
    "# Download and create training and test datasets\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) # Create training dataset\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) # Create test dataset\n",
    "\n",
    "# Create DataLoader instances to efficiently load and iterate over batches of data\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Step 3: Initialize the pre-trained DenseNet model\n",
    "# Initialize the DenseNet121 model with pre-trained weights from the ImageNet dataset\n",
    "model = models.densenet121(weights='IMAGENET1K_V1')\n",
    "# Modify the classifier for CIFAR-10 (10 classes)\n",
    "model.classifier = nn.Linear(1024, 10)  # Change the output layer to have 10 classes for CIFAR-10\n",
    "model = model.to(device)  # Move the model to the GPU (if available)\n",
    "\n",
    "# Step 4: Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer with specified learning rate\n",
    "\n",
    "# Creating a function to calculate uncertainty for each epoch metrics\n",
    "def calculate_uncertainty_metrics(outputs):\n",
    "    # Convert NumPy array to PyTorch tensor\n",
    "    outputs_tensor = torch.from_numpy(outputs)\n",
    "    # Applying softmax along dimension 1\n",
    "    probabilities = F.softmax(outputs_tensor, dim=1)\n",
    "    # Least Confidence: 1 - Maximum probability for each sample\n",
    "    least_confidence = 1 - probabilities.max(dim=1).values.cpu().detach().numpy()\n",
    "    # Handling NaN in prediction entropy\n",
    "    current_probs = probabilities.clone().detach()\n",
    "    current_probs[current_probs == 0] = 1e-10  # Adding a small epsilon to avoid log(0)\n",
    "    # Prediction Entropy: Negative sum of (probability * log(probability)) for each class\n",
    "    prediction_entropy = -torch.sum(current_probs * torch.log(current_probs), dim=1).cpu().detach().numpy()\n",
    "    # Margin Sampling: 1 - (Maximum probability - Minimum probability) for each sample\n",
    "    margin_sampling = 1 - torch.max(probabilities, dim=1).values.cpu().detach().numpy() - \\\n",
    "                      torch.min(probabilities, dim=1).values.cpu().detach().numpy()\n",
    "    # Returning least confidence,prediction entropy and margin sampling values obtained\n",
    "    return least_confidence, prediction_entropy, margin_sampling\n",
    "\n",
    "# Creating a function to caalculate diversiy metrics\n",
    "def calculate_diversity_metrics(features, m=5):\n",
    "    # Calculate pairwise distances using cosine similarity\n",
    "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
    "    # Cosine Similarity: 1 - Mean cosine similarity with the top m neighbors for each sample\n",
    "    cosine_similarity = 1 - feature_distances[:, 1:m+1].mean(axis=1)\n",
    "    # Calculate pairwise distances using L2 (Euclidean) norm\n",
    "    l2_distances = pairwise_distances(features.cpu().detach().numpy(), metric='euclidean')\n",
    "    # L2 Norm: Mean L2 norm with the top m neighbors for each sample\n",
    "    l2_norm = l2_distances[:, 1:m+1].mean(axis=1)\n",
    "    # Returning cosine similarity and l2 norm values obtained \n",
    "    return cosine_similarity, l2_norm\n",
    "\n",
    "# Creating a function to calculate kl divergence \n",
    "def calculate_kl_divergence(outputs, feature_distances, m=5):\n",
    "    # List to store KL divergence scores for each sample\n",
    "    kl_divergence = []\n",
    "    # Iterate over each sample in the outputs\n",
    "    for i in range(len(outputs)):\n",
    "        # Calculate the probability distribution of the current sample\n",
    "        current_sample_prob = F.softmax(outputs[i], dim=0)\n",
    "        # Get the indices of the top m neighbors for the current sample\n",
    "        neighbor_indices = feature_distances[i, 1:m+1].astype(int)\n",
    "        # Calculate the average probability distribution of the top m neighbors for the current sample \n",
    "        neighbors_prob = torch.mean(F.softmax(outputs[neighbor_indices], dim=1), dim=0)\n",
    "\n",
    "        # Add a small epsilon to avoid zero probabilities\n",
    "        epsilon = 1e-10\n",
    "        current_sample_prob = current_sample_prob + epsilon # epsilon is the  probability that the current sample corresponds to the current sample probability in the distribution function .\n",
    "        neighbors_prob = neighbors_prob + epsilon \n",
    "\n",
    "        # Calculate KL divergence between the current sample and its neighbors\n",
    "        kl_divergence.append(F.kl_div(torch.log(current_sample_prob), neighbors_prob, reduction='batchmean'))\n",
    "    \n",
    "    # Returning kl divergence values obtained\n",
    "    return kl_divergence\n",
    "\n",
    "# Creating a Function to calculate uncertainty and diversity metrics\n",
    "def calculate_metrics(outputs, features, m=5):\n",
    "    # Calculate uncertainty metrics\n",
    "    least_confidence, prediction_entropy, margin_sampling = calculate_uncertainty_metrics(outputs.detach().numpy())\n",
    "    # Extend lists with uncertainty metrics\n",
    "    least_confidence_list.extend(torch.from_numpy(least_confidence))\n",
    "    prediction_entropy_list.extend(torch.from_numpy(prediction_entropy))\n",
    "    margin_sampling_list.extend(torch.from_numpy(margin_sampling))\n",
    "    # Calculate diversity metrics\n",
    "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
    "    features_normalized = F.normalize(features, p=2, dim=1)\n",
    "    cosine_similarity, l2_norm = calculate_diversity_metrics(features_normalized)\n",
    "    # cosine_similarity, l2_norm = calculate_diversity_metrics(features)\n",
    "    # Extend lists with diversity metrics\n",
    "    cosine_similarity_list.extend(torch.from_numpy(cosine_similarity))\n",
    "    # Inside the calculate_metrics function\n",
    "    l2_norm_list.extend(torch.from_numpy(l2_norm))\n",
    "    # Calculate KL divergence scores\n",
    "    kl_divergence_scores = calculate_kl_divergence(outputs, feature_distances, m=5)\n",
    "    # Extend the list with KL divergence scores\n",
    "    kl_divergence_list.extend(kl_divergence_scores)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs): # Iterate over each epoch\n",
    "    model.train()  # Set the model to training mode\n",
    "    for images, labels in train_loader: # Iterate through batches of training data\n",
    "        images, labels = images.to(device), labels.to(device) # Move data to GPU if available\n",
    "        optimizer.zero_grad()  # Zero gradients to clear previous gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass to compute gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")  # Print the loss for the current epoch\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval() # Set the model to evaluation mode\n",
    "correct = 0  # Initialize the number of correctly predicted samples\n",
    "total = 0  # Initialize the total number of samples\n",
    "\n",
    "least_confidence_list = [] # Initialize an empty list to store least confidence values\n",
    "prediction_entropy_list = [] # Initialize an empty list to store prediction entropy values\n",
    "margin_sampling_list = [] # Initialize an empty list to store margin sampling values\n",
    "cosine_similarity_list = [] # Initialize an empty list to store cosine similarity values\n",
    "l2_norm_list = [] # Initialize an empty list to store L2 norm values\n",
    "kl_divergence_list = [] # Initialize an empty list to store KL Divergence values\n",
    "\n",
    "# Use torch.no_grad() to disable gradient computation during testing\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader: # Iterate over batches in the test loader\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU if available\n",
    "        outputs = model(images)  # Forward pass\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predicted class labels\n",
    "        total += labels.size(0)  # Increment total number of images\n",
    "        correct += (predicted == labels).sum().item()  # Count correctly predicted images\n",
    "\n",
    "        # Diversity measures\n",
    "        features = model.features(images)  # Use the feature extraction part of the model\n",
    "        features = F.adaptive_avg_pool2d(features, (1, 1))  # Global average pooling\n",
    "        features = features.view(features.size(0), -1)  # Flatten the features for distance calculations\n",
    "\n",
    "        calculate_metrics(outputs, features)\n",
    "\n",
    "accuracy = correct / total  # Calculate accuracy\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")  # Print test accuracy\n",
    "\n",
    "# Print the average values of uncertainty and diversity measures\n",
    "print(f\"Average Least Confidence: {torch.mean(torch.stack(least_confidence_list))}\") # Print the average value of least confidence\n",
    "print(f\"Average Prediction Entropy: {torch.mean(torch.stack(prediction_entropy_list))}\") # Print the average value of prediction entropy\n",
    "print(f\"Average Margin Sampling: {torch.mean(torch.stack(margin_sampling_list))}\") # Print the average value of margin sampling\n",
    "print(f\"Average Cosine Similarity: {torch.mean(torch.stack(cosine_similarity_list))}\") # Print the average value of cosine similarity\n",
    "print(f\"Average L2 Norm: {torch.mean(torch.stack(l2_norm_list))}\") # Print the average value of L2 norm\n",
    "print(f\"Average KL Divergence: {torch.mean(torch.stack(kl_divergence_list))}\") # Print the average value of KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY 5: USING ANOTHER PRETRAINED MODEL: RESNET 50 FOR BETTER ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/10, Loss: 0.3159460723400116\n",
      "Epoch 2/10, Loss: 0.6475083827972412\n",
      "Epoch 3/10, Loss: 0.2746032178401947\n",
      "Epoch 4/10, Loss: 0.7377022504806519\n",
      "Epoch 5/10, Loss: 0.736390233039856\n",
      "Epoch 6/10, Loss: 0.8800309300422668\n",
      "Epoch 7/10, Loss: 0.4674454927444458\n",
      "Epoch 8/10, Loss: 0.9689621925354004\n",
      "Epoch 9/10, Loss: 0.44302937388420105\n",
      "Epoch 10/10, Loss: 0.3406006991863251\n",
      "Test Accuracy: 84.41%\n",
      "Average Least Confidence: 0.0828772708773613\n",
      "Average Prediction Entropy: 0.32879531383514404\n",
      "Average Margin Sampling: 0.11347316950559616\n",
      "Average Cosine Similarity: 0.6799415349960327\n",
      "Average L2 Norm: 0.6122885346412659\n",
      "Average KL Divergence: 0.3962635397911072\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries \n",
    "# This includes the essential PyTorch and torchvision modules for working with neural networks and datasets.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Step 1: Set device and hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check if GPU is available, else use CPU\n",
    "batch_size = 64  # Number of images in each mini-batch\n",
    "learning_rate = 0.001  # Learning rate for the optimizer\n",
    "epochs = 10 # Number of times to iterate through the entire dataset during training\n",
    "m = 5  # Number of nearest neighbors for diversity measures \n",
    "\n",
    "# Step 2: Load and preprocess the CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image pixel values\n",
    "])\n",
    "\n",
    "# Download and create training and test datasets\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) # Create training dataset\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) # Create test dataset\n",
    "\n",
    "# Create DataLoader instances to efficiently load and iterate over batches of data\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Step 3: Initialize the pre-trained DenseNet model\n",
    "# Initialize the DenseNet121 model with pre-trained weights from the ImageNet dataset\n",
    "# model = models.efficientnet_v2_m(weights='IMAGENET1K_V1')\n",
    "model = models.resnet50(weights='IMAGENET1K_V2')\n",
    "# Modify the classifier for CIFAR-10 (10 classes)\n",
    "model.classifier = nn.Linear(1024, 10)  # Change the output layer to have 10 classes for CIFAR-10\n",
    "model = model.to(device)  # Move the model to the GPU (if available)\n",
    "\n",
    "# Step 4: Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer with specified learning rate\n",
    "\n",
    "# Creating a function to calculate uncertainty for each epoch metrics\n",
    "def calculate_uncertainty_metrics(outputs):\n",
    "    # Convert NumPy array to PyTorch tensor\n",
    "    outputs_tensor = torch.from_numpy(outputs)\n",
    "    # Applying softmax along dimension 1\n",
    "    probabilities = F.softmax(outputs_tensor, dim=1)\n",
    "    # Least Confidence: 1 - Maximum probability for each sample\n",
    "    least_confidence = 1 - probabilities.max(dim=1).values.cpu().detach().numpy()\n",
    "    # Handling NaN in prediction entropy\n",
    "    current_probs = probabilities.clone().detach()\n",
    "    current_probs[current_probs == 0] = 1e-10  # Adding a small epsilon to avoid log(0)\n",
    "    # Prediction Entropy: Negative sum of (probability * log(probability)) for each class\n",
    "    prediction_entropy = -torch.sum(current_probs * torch.log(current_probs), dim=1).cpu().detach().numpy()\n",
    "    # Margin Sampling: 1 - (Maximum probability - Minimum probability) for each sample\n",
    "    margin_sampling = 1 - torch.max(probabilities, dim=1).values.cpu().detach().numpy() - \\\n",
    "                      torch.min(probabilities, dim=1).values.cpu().detach().numpy()\n",
    "    # Returning least confidence,prediction entropy and margin sampling values obtained\n",
    "    return least_confidence, prediction_entropy, margin_sampling\n",
    "\n",
    "# Creating a function to caalculate diversiy metrics\n",
    "def calculate_diversity_metrics(features, m=5):\n",
    "    # Calculate pairwise distances using cosine similarity\n",
    "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
    "    # Cosine Similarity: 1 - Mean cosine similarity with the top m neighbors for each sample\n",
    "    cosine_similarity = 1 - feature_distances[:, 1:m+1].mean(axis=1)\n",
    "    # Calculate pairwise distances using L2 (Euclidean) norm\n",
    "    l2_distances = pairwise_distances(features.cpu().detach().numpy(), metric='euclidean')\n",
    "    # L2 Norm: Mean L2 norm with the top m neighbors for each sample\n",
    "    l2_norm = l2_distances[:, 1:m+1].mean(axis=1)\n",
    "    # Returning cosine similarity and l2 norm values obtained \n",
    "    return cosine_similarity, l2_norm\n",
    "\n",
    "# Creating a function to calculate kl divergence \n",
    "def calculate_kl_divergence(outputs, feature_distances, m=5):\n",
    "    # List to store KL divergence scores for each sample\n",
    "    kl_divergence = []\n",
    "    # Iterate over each sample in the outputs\n",
    "    for i in range(len(outputs)):\n",
    "        # Calculate the probability distribution of the current sample\n",
    "        current_sample_prob = F.softmax(outputs[i], dim=0)\n",
    "        # Get the indices of the top m neighbors for the current sample\n",
    "        neighbor_indices = feature_distances[i, 1:m+1].astype(int)\n",
    "        # Calculate the average probability distribution of the neighbors\n",
    "        neighbors_prob = torch.mean(F.softmax(outputs[neighbor_indices], dim=1), dim=0)\n",
    "\n",
    "        # Add a small epsilon to avoid zero probabilities\n",
    "        epsilon = 1e-10\n",
    "        current_sample_prob = current_sample_prob + epsilon\n",
    "        neighbors_prob = neighbors_prob + epsilon\n",
    "\n",
    "        # Calculate KL divergence between the current sample and its neighbors\n",
    "        kl_divergence.append(F.kl_div(torch.log(current_sample_prob), neighbors_prob, reduction='batchmean'))\n",
    "    \n",
    "    # Returning kl divergence values obtained\n",
    "    return kl_divergence\n",
    "\n",
    "# Creating a Function to calculate uncertainty and diversity metrics\n",
    "def calculate_metrics(outputs, features, m=5):\n",
    "    # Calculate uncertainty metrics\n",
    "    least_confidence, prediction_entropy, margin_sampling = calculate_uncertainty_metrics(outputs.detach().numpy())\n",
    "    # Extend lists with uncertainty metrics\n",
    "    least_confidence_list.extend(torch.from_numpy(least_confidence))\n",
    "    prediction_entropy_list.extend(torch.from_numpy(prediction_entropy))\n",
    "    margin_sampling_list.extend(torch.from_numpy(margin_sampling))\n",
    "    # Calculate diversity metrics\n",
    "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
    "    features_normalized = F.normalize(features, p=2, dim=1)\n",
    "    cosine_similarity, l2_norm = calculate_diversity_metrics(features_normalized)\n",
    "    # cosine_similarity, l2_norm = calculate_diversity_metrics(features)\n",
    "    # Extend lists with diversity metrics\n",
    "    cosine_similarity_list.extend(torch.from_numpy(cosine_similarity))\n",
    "    # Inside the calculate_metrics function\n",
    "    l2_norm_list.extend(torch.from_numpy(l2_norm))\n",
    "    # Calculate KL divergence scores\n",
    "    kl_divergence_scores = calculate_kl_divergence(outputs, feature_distances, m=5)\n",
    "    # Extend the list with KL divergence scores\n",
    "    kl_divergence_list.extend(kl_divergence_scores)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs): # Iterate over each epoch\n",
    "    model.train()  # Set the model to training mode\n",
    "    for images, labels in train_loader: # Iterate through batches of training data\n",
    "        images, labels = images.to(device), labels.to(device) # Move data to GPU if available\n",
    "        optimizer.zero_grad()  # Zero gradients to clear previous gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass to compute gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")  # Print the loss for the current epoch\"\"\"\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval() # Set the model to evaluation mode\n",
    "correct = 0  # Initialize the number of correctly predicted samples\n",
    "total = 0  # Initialize the total number of samples\n",
    "\n",
    "least_confidence_list = [] # Initialize an empty list to store least confidence values\n",
    "prediction_entropy_list = [] # Initialize an empty list to store prediction entropy values\n",
    "margin_sampling_list = [] # Initialize an empty list to store margin sampling values\n",
    "cosine_similarity_list = [] # Initialize an empty list to store cosine similarity values\n",
    "l2_norm_list = [] # Initialize an empty list to store L2 norm values\n",
    "kl_divergence_list = [] # Initialize an empty list to store KL Divergence values\n",
    "\n",
    "# Use torch.no_grad() to disable gradient computation during testing\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader: # Iterate over batches in the test loader\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU if available\n",
    "        outputs = model(images)  # Forward pass\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predicted class labels\n",
    "        total += labels.size(0)  # Increment total number of images\n",
    "        correct += (predicted == labels).sum().item()  # Count correctly predicted images\n",
    "        # Diversity measures\n",
    "\n",
    "        # Extract features from the model layers\n",
    "        features = model.conv1(images)  # Apply convolutional layer 1 to the input images\n",
    "        features = model.relu(features) # Apply Rectified Linear Unit (ReLU) activation function to introduce non-linearity\n",
    "        features = model.maxpool(features) # Apply max pooling to reduce spatial dimensions and retain important information\n",
    "        features = model.layer1(features)  # Use layer1 instead of conv2 for ResNet\n",
    "        features = model.layer2(features)  # Use layer2 instead of conv2 for ResNet\n",
    "        features = model.avgpool(features)  # Use avgpool instead of maxpool for ResNet\n",
    "\n",
    "        # Flatten the features for distance calculations\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        calculate_metrics(outputs, features) # Calculate metrics function call \n",
    "\n",
    "accuracy = correct / total  # Calculate accuracy\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")  # Print test accuracy\n",
    "\n",
    "# Print the average values of uncertainty and diversity measures\n",
    "print(f\"Average Least Confidence: {torch.mean(torch.stack(least_confidence_list))}\") # Print the average value of least confidence\n",
    "print(f\"Average Prediction Entropy: {torch.mean(torch.stack(prediction_entropy_list))}\") # Print the average value of prediction entropy\n",
    "print(f\"Average Margin Sampling: {torch.mean(torch.stack(margin_sampling_list))}\") # Print the average value of margin sampling\n",
    "print(f\"Average Cosine Similarity: {torch.mean(torch.stack(cosine_similarity_list))}\") # Print the average value of cosine similarity\n",
    "print(f\"Average L2 Norm: {torch.mean(torch.stack(l2_norm_list))}\") # Print the average value of L2 norm\n",
    "print(f\"Average KL Divergence: {torch.mean(torch.stack(kl_divergence_list))}\") # Print the average value of KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY 6 : USINNG ANOTHER PRETRAINED MODEL : RESNET152 FOR ACHIEVING BETTER ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/5, Loss: 1.2921116352081299\n",
      "Epoch 2/5, Loss: 0.9806951284408569\n",
      "Epoch 3/5, Loss: 0.3718707263469696\n",
      "Epoch 4/5, Loss: 0.11784355342388153\n",
      "Epoch 5/5, Loss: 0.47144776582717896\n",
      "Test Accuracy: 82.18%\n",
      "Average Least Confidence: 0.12839721143245697\n",
      "Average Prediction Entropy: 0.3821943402290344\n",
      "Average Margin Sampling: 0.1283971667289734\n",
      "Average Cosine Similarity: 0.9807947278022766\n",
      "Average L2 Norm: 0.18331636488437653\n",
      "Average KL Divergence: 0.007689288351684809\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries \n",
    "# This includes the essential PyTorch and torchvision modules for working with neural networks and datasets.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Step 1: Set device and hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check if GPU is available, else use CPU\n",
    "batch_size = 64  # Number of images in each mini-batch\n",
    "learning_rate = 0.001  # Learning rate for the optimizer\n",
    "epochs = 5 # Number of times to iterate through the entire dataset during training\n",
    "m = 5  # Number of nearest neighbors for diversity measures \n",
    "\n",
    "# Step 2: Load and preprocess the CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image pixel values\n",
    "])\n",
    "\n",
    "# Download and create training and test datasets\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) # Create training dataset\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) # Create test dataset\n",
    "\n",
    "# Create DataLoader instances to efficiently load and iterate over batches of data\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Step 3: Initialize the pre-trained DenseNet model\n",
    "# Initialize the DenseNet121 model with pre-trained weights from the ImageNet dataset\n",
    "# model = models.efficientnet_v2_m(weights='IMAGENET1K_V1')\n",
    "model = models.resnet152(weights='IMAGENET1K_V2')\n",
    "# Modify the classifier for CIFAR-10 (10 classes)\n",
    "model.classifier = nn.Linear(1024, 10)  # Change the output layer to have 10 classes for CIFAR-10\n",
    "model = model.to(device)  # Move the model to the GPU (if available)\n",
    "\n",
    "# Step 4: Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer with specified learning rate\n",
    "\n",
    "# Creating a function to calculate uncertainty for each epoch metrics\n",
    "def calculate_uncertainty_metrics(outputs):\n",
    "    # Convert NumPy array to PyTorch tensor\n",
    "    outputs_tensor = torch.from_numpy(outputs)\n",
    "    # Applying softmax along dimension 1\n",
    "    probabilities = F.softmax(outputs_tensor, dim=1)\n",
    "    # Least Confidence: 1 - Maximum probability for each sample\n",
    "    least_confidence = 1 - probabilities.max(dim=1).values.cpu().detach().numpy()\n",
    "    # Handling NaN in prediction entropy\n",
    "    current_probs = probabilities.clone().detach()\n",
    "    current_probs[current_probs == 0] = 1e-10  # Adding a small epsilon to avoid log(0)\n",
    "    # Prediction Entropy: Negative sum of (probability * log(probability)) for each class\n",
    "    prediction_entropy = -torch.sum(current_probs * torch.log(current_probs), dim=1).cpu().detach().numpy()\n",
    "    # Margin Sampling: 1 - (Maximum probability - Minimum probability) for each sample\n",
    "    margin_sampling = 1 - torch.max(probabilities, dim=1).values.cpu().detach().numpy() - \\\n",
    "                      torch.min(probabilities, dim=1).values.cpu().detach().numpy()\n",
    "    # Returning least confidence,prediction entropy and margin sampling values obtained\n",
    "    return least_confidence, prediction_entropy, margin_sampling\n",
    "\n",
    "# Creating a function to caalculate diversiy metrics\n",
    "def calculate_diversity_metrics(features, m=5):\n",
    "    # Calculate pairwise distances using cosine similarity\n",
    "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
    "    # Cosine Similarity: 1 - Mean cosine similarity with the top m neighbors for each sample\n",
    "    cosine_similarity = 1 - feature_distances[:, 1:m+1].mean(axis=1)\n",
    "    # Calculate pairwise distances using L2 (Euclidean) norm\n",
    "    l2_distances = pairwise_distances(features.cpu().detach().numpy(), metric='euclidean')\n",
    "    # L2 Norm: Mean L2 norm with the top m neighbors for each sample\n",
    "    l2_norm = l2_distances[:, 1:m+1].mean(axis=1)\n",
    "    # Returning cosine similarity and l2 norm values obtained \n",
    "    return cosine_similarity, l2_norm\n",
    "\n",
    "# Creating a function to calculate kl divergence \n",
    "def calculate_kl_divergence(outputs, feature_distances, m=5):\n",
    "    # List to store KL divergence scores for each sample\n",
    "    kl_divergence = []\n",
    "    # Iterate over each sample in the outputs\n",
    "    for i in range(len(outputs)):\n",
    "        # Calculate the probability distribution of the current sample\n",
    "        current_sample_prob = F.softmax(outputs[i], dim=0)\n",
    "        # Get the indices of the top m neighbors for the current sample\n",
    "        neighbor_indices = feature_distances[i, 1:m+1].astype(int)\n",
    "        # Calculate the average probability distribution of the neighbors\n",
    "        neighbors_prob = torch.mean(F.softmax(outputs[neighbor_indices], dim=1), dim=0)\n",
    "\n",
    "        # Add a small epsilon to avoid zero probabilities\n",
    "        epsilon = 1e-10\n",
    "        current_sample_prob = current_sample_prob + epsilon\n",
    "        neighbors_prob = neighbors_prob + epsilon\n",
    "\n",
    "        # Calculate KL divergence between the current sample and its neighbors\n",
    "        kl_divergence.append(F.kl_div(torch.log(current_sample_prob), neighbors_prob, reduction='batchmean'))\n",
    "    \n",
    "    # Returning kl divergence values obtained\n",
    "    return kl_divergence\n",
    "\n",
    "# Creating a Function to calculate uncertainty and diversity metrics\n",
    "def calculate_metrics(outputs, features, m=5):\n",
    "    # Calculate uncertainty metrics\n",
    "    least_confidence, prediction_entropy, margin_sampling = calculate_uncertainty_metrics(outputs.detach().numpy())\n",
    "    # Extend lists with uncertainty metrics\n",
    "    least_confidence_list.extend(torch.from_numpy(least_confidence))\n",
    "    prediction_entropy_list.extend(torch.from_numpy(prediction_entropy))\n",
    "    margin_sampling_list.extend(torch.from_numpy(margin_sampling))\n",
    "    # Calculate diversity metrics\n",
    "    feature_distances = pairwise_distances(features.cpu().detach().numpy(), metric='cosine')\n",
    "    features_normalized = F.normalize(features, p=2, dim=1)\n",
    "    cosine_similarity, l2_norm = calculate_diversity_metrics(features_normalized)\n",
    "    # cosine_similarity, l2_norm = calculate_diversity_metrics(features)\n",
    "    # Extend lists with diversity metrics\n",
    "    cosine_similarity_list.extend(torch.from_numpy(cosine_similarity))\n",
    "    # Inside the calculate_metrics function\n",
    "    l2_norm_list.extend(torch.from_numpy(l2_norm))\n",
    "    # Calculate KL divergence scores\n",
    "    kl_divergence_scores = calculate_kl_divergence(outputs, feature_distances, m=5)\n",
    "    # Extend the list with KL divergence scores\n",
    "    kl_divergence_list.extend(kl_divergence_scores)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs): # Iterate over each epoch\n",
    "    model.train()  # Set the model to training mode\n",
    "    for images, labels in train_loader: # Iterate through batches of training data\n",
    "        images, labels = images.to(device), labels.to(device) # Move data to GPU if available\n",
    "        optimizer.zero_grad()  # Zero gradients to clear previous gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass to compute gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")  # Print the loss for the current epoch\"\"\"\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval() # Set the model to evaluation mode\n",
    "correct = 0  # Initialize the number of correctly predicted samples\n",
    "total = 0  # Initialize the total number of samples\n",
    "\n",
    "least_confidence_list = [] # Initialize an empty list to store least confidence values\n",
    "prediction_entropy_list = [] # Initialize an empty list to store prediction entropy values\n",
    "margin_sampling_list = [] # Initialize an empty list to store margin sampling values\n",
    "cosine_similarity_list = [] # Initialize an empty list to store cosine similarity values\n",
    "l2_norm_list = [] # Initialize an empty list to store L2 norm values\n",
    "kl_divergence_list = [] # Initialize an empty list to store KL Divergence values\n",
    "\n",
    "# Use torch.no_grad() to disable gradient computation during testing\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader: # Iterate over batches in the test loader\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU if available\n",
    "        outputs = model(images)  # Forward pass\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predicted class labels\n",
    "        total += labels.size(0)  # Increment total number of images\n",
    "        correct += (predicted == labels).sum().item()  # Count correctly predicted images\n",
    "        \n",
    "        # Diversity measures\n",
    "        # Extract features from the model layers\n",
    "        features = model.conv1(images)  # Apply convolutional layer 1 to the input images\n",
    "        features = model.relu(features) # Apply Rectified Linear Unit (ReLU) activation function to introduce non-linearity\n",
    "        features = model.maxpool(features) # Apply max pooling to reduce spatial dimensions and retain important information\n",
    "        features = model.layer1(features)  # Use layer1 instead of conv2 for ResNet\n",
    "        features = model.layer2(features)  # Use layer2 instead of conv2 for ResNet\n",
    "        features = model.avgpool(features)  # Use avgpool instead of maxpool for ResNet\n",
    "\n",
    "        # Flatten the features for distance calculations\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        calculate_metrics(outputs, features) # Calculate metrics function call \n",
    "\n",
    "accuracy = correct / total  # Calculate accuracy\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")  # Print test accuracy\n",
    "\n",
    "# Print the average values of uncertainty and diversity measures\n",
    "print(f\"Average Least Confidence: {torch.mean(torch.stack(least_confidence_list))}\") # Print the average value of least confidence\n",
    "print(f\"Average Prediction Entropy: {torch.mean(torch.stack(prediction_entropy_list))}\") # Print the average value of prediction entropy\n",
    "print(f\"Average Margin Sampling: {torch.mean(torch.stack(margin_sampling_list))}\") # Print the average value of margin sampling\n",
    "print(f\"Average Cosine Similarity: {torch.mean(torch.stack(cosine_similarity_list))}\") # Print the average value of cosine similarity\n",
    "print(f\"Average L2 Norm: {torch.mean(torch.stack(l2_norm_list))}\") # Print the average value of L2 norm\n",
    "print(f\"Average KL Divergence: {torch.mean(torch.stack(kl_divergence_list))}\") # Print the average value of KL divergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
