{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTATION OF IMAGE CLASSIFIER OF DATASET : CIFAR-10 USING PYTORCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRY 1: USING NEURAL NETWORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[1,  2000] loss: 2.227\n",
      "[1,  4000] loss: 1.848\n",
      "[1,  6000] loss: 1.671\n",
      "[1,  8000] loss: 1.573\n",
      "[1, 10000] loss: 1.519\n",
      "[1, 12000] loss: 1.462\n",
      "[2,  2000] loss: 1.420\n",
      "[2,  4000] loss: 1.357\n",
      "[2,  6000] loss: 1.330\n",
      "[2,  8000] loss: 1.336\n",
      "[2, 10000] loss: 1.293\n",
      "[2, 12000] loss: 1.316\n",
      "[3,  2000] loss: 1.235\n",
      "[3,  4000] loss: 1.209\n",
      "[3,  6000] loss: 1.211\n",
      "[3,  8000] loss: 1.203\n",
      "[3, 10000] loss: 1.185\n",
      "[3, 12000] loss: 1.188\n",
      "[4,  2000] loss: 1.114\n",
      "[4,  4000] loss: 1.118\n",
      "[4,  6000] loss: 1.126\n",
      "[4,  8000] loss: 1.126\n",
      "[4, 10000] loss: 1.118\n",
      "[4, 12000] loss: 1.103\n",
      "[5,  2000] loss: 1.033\n",
      "[5,  4000] loss: 1.051\n",
      "[5,  6000] loss: 1.051\n",
      "[5,  8000] loss: 1.056\n",
      "[5, 10000] loss: 1.073\n",
      "[5, 12000] loss: 1.046\n",
      "[6,  2000] loss: 0.977\n",
      "[6,  4000] loss: 0.988\n",
      "[6,  6000] loss: 0.979\n",
      "[6,  8000] loss: 1.000\n",
      "[6, 10000] loss: 1.018\n",
      "[6, 12000] loss: 1.013\n",
      "[7,  2000] loss: 0.917\n",
      "[7,  4000] loss: 0.944\n",
      "[7,  6000] loss: 0.941\n",
      "[7,  8000] loss: 0.967\n",
      "[7, 10000] loss: 0.978\n",
      "[7, 12000] loss: 0.957\n",
      "[8,  2000] loss: 0.864\n",
      "[8,  4000] loss: 0.877\n",
      "[8,  6000] loss: 0.915\n",
      "[8,  8000] loss: 0.923\n",
      "[8, 10000] loss: 0.932\n",
      "[8, 12000] loss: 0.952\n",
      "[9,  2000] loss: 0.837\n",
      "[9,  4000] loss: 0.856\n",
      "[9,  6000] loss: 0.866\n",
      "[9,  8000] loss: 0.884\n",
      "[9, 10000] loss: 0.897\n",
      "[9, 12000] loss: 0.918\n",
      "[10,  2000] loss: 0.780\n",
      "[10,  4000] loss: 0.842\n",
      "[10,  6000] loss: 0.834\n",
      "[10,  8000] loss: 0.847\n",
      "[10, 10000] loss: 0.864\n",
      "[10, 12000] loss: 0.892\n",
      "Accuracy on the test set: 62.93%\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "# This includes the essential PyTorch and torchvision modules for working with neural networks and datasets.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image pixel values\n",
    "])\n",
    "\n",
    "# Download and load the CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) # Create training dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) # Create test dataset\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# Define the classes for classification\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class (nn.Module) to initialize the base class\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)  # Input channels: 3, Output channels: 6, Kernel size: 5x5\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Max pooling layer with kernel size 2x2 and stride 2\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  # Input channels: 6, Output channels: 16, Kernel size: 5x5\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Flatten layer to convert 3D tensor to 1D tensor before fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # Input features: 16*5*5, Output features: 120\n",
    "        self.fc2 = nn.Linear(120, 84)  # Input features: 120, Output features: 84\n",
    "        self.fc3 = nn.Linear(84, 10)  # Input features: 84, Output features: 10 (number of classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "\n",
    "        # First convolutional layer (conv1) followed by ReLU activation and max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Second convolutional layer (conv2) followed by ReLU activation and max pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten the output to a 1D tensor before passing it to fully connected layers\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        # First fully connected layer (fc1) followed by ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Second fully connected layer (fc2) followed by ReLU activation\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Final fully connected layer (fc3) for classification output\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x # Return the final output \n",
    "    \n",
    "# Instantiate the network\n",
    "net = Net()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # SGD optimizer with momentum\n",
    "epochs = 10  # Number of times to iterate through the entire dataset during training\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):  # Loop over the dataset for a specified number of epochs\n",
    "    running_loss = 0.0  # Initialize the running loss for the current epoch\n",
    "    for i, data in enumerate(trainloader, 0):  # Iterate over batches in the training loader\n",
    "        inputs, labels = data  # Get inputs and labels for the current batch\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients to clear previous gradients\n",
    "\n",
    "        outputs = net(inputs)  # Forward pass to compute the predicted outputs\n",
    "        loss = criterion(outputs, labels)  # Compute the loss between predicted and true labels\n",
    "        loss.backward()  # Backward pass to compute gradients of the loss with respect to model parameters\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "\n",
    "        running_loss += loss.item()  # Accumulate the running loss for statistics\n",
    "        if i % 2000 == 1999:  # Print every 2000 batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0  # Reset the running loss for the next set of batches\n",
    "\n",
    "# Testing the model\n",
    "correct = 0  # Initialize the number of correctly predicted samples\n",
    "total = 0  # Initialize the total number of samples\n",
    "\n",
    "# Use torch.no_grad() to disable gradient computation during testing\n",
    "with torch.no_grad():\n",
    "    for data in testloader:  # Iterate over batches in the test loader\n",
    "        images, labels = data  # Get inputs and true labels for the current batch\n",
    "        outputs = net(images)  # Forward pass to compute predicted outputs\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get the index of the maximum predicted value\n",
    "        total += labels.size(0)  # Increment the total number of samples by the batch size\n",
    "        correct += (predicted == labels).sum().item()  # Count the number of correctly predicted samples\n",
    "\n",
    "accuracy = 100 * correct / total # Calculate accuracy\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}%\") # Print the accuracy on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRY 2: USING CNN NEURAL NETWORK AND DOING MODIFACTIONS FOR BETTER ACCURACY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/10, Loss: 0.9492315053939819\n",
      "Epoch 2/10, Loss: 1.301919937133789\n",
      "Epoch 3/10, Loss: 0.4410093128681183\n",
      "Epoch 4/10, Loss: 0.4600154757499695\n",
      "Epoch 5/10, Loss: 0.3084782361984253\n",
      "Epoch 6/10, Loss: 0.38129258155822754\n",
      "Epoch 7/10, Loss: 0.21933627128601074\n",
      "Epoch 8/10, Loss: 0.09921213984489441\n",
      "Epoch 9/10, Loss: 0.006839558482170105\n",
      "Epoch 10/10, Loss: 0.14239855110645294\n",
      "Test Accuracy: 72.15%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "# This includes the essential PyTorch and torchvision modules for working with neural networks and datasets.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "    \n",
    "        # Convolutional layer 1: Input channels=3, output channels=32, kernel size=3, padding=1\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        # Rectified Linear Unit (ReLU) activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # Max pooling layer 1: Kernel size=2, stride=2\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        # Convolutional layer 2: Input channels=32, output channels=64, kernel size=3, padding=1\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        # Flatten layer to convert 3D tensor to 1D tensor\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Fully connected layer 1: Input features=64*8*8, output features=512\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        # Fully connected layer 2: Input features=512, output features=10 (output classes)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Convolutional layer 1: Input tensor x undergoes convolution\n",
    "        x = self.conv1(x)\n",
    "        # Apply ReLU activation function to introduce non-linearity\n",
    "        x = self.relu(x)\n",
    "        # Perform max pooling to down-sample the spatial dimensions\n",
    "        x = self.maxpool(x)\n",
    "    \n",
    "        # Convolutional layer 2: Apply another convolution operation\n",
    "        x = self.conv2(x)\n",
    "        # Apply ReLU activation\n",
    "        x = self.relu(x)\n",
    "        # Another max pooling operation\n",
    "        x = self.maxpool(x)\n",
    "    \n",
    "        # Flatten the tensor to prepare for fully connected layers\n",
    "        x = self.flatten(x)\n",
    "    \n",
    "        # Fully connected layer 1: Apply linear transformation\n",
    "        x = self.fc1(x)\n",
    "        # Apply ReLU activation\n",
    "        x = self.relu(x)\n",
    "    \n",
    "        # Fully connected layer 2: Produce the final output\n",
    "        x = self.fc2(x)\n",
    "    \n",
    "        # Return the final output tensor after passing through the network\n",
    "        return x\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Hyperparameters\n",
    "batch_size = 64 # Set the batch size for training\n",
    "learning_rate = 0.001 # Set the learning rate for the optimizer\n",
    "epochs = 10 # Set the number of training epochs\n",
    "\n",
    "# Define data transformation for CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize pixel values\n",
    "])\n",
    "\n",
    "# Download and Load CIFAR-10 training and test datasets\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)  # Create training dataset\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)  # Create test dataset\n",
    "\n",
    "# Create DataLoader instances for training and test datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the neural network model, loss function, and optimizer\n",
    "model = SimpleCNN().to(device)  # Move the model to GPU if available\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs): # Iterate over each epoch\n",
    "    model.train()  # Set the model to training mode\n",
    "    for images, labels in train_loader: # Iterate through batches of training data\n",
    "        images, labels = images.to(device), labels.to(device) # Move data to GPU if available\n",
    "        optimizer.zero_grad()  # Zero gradients to clear previous gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass to compute gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")  # Print the loss for the current epoch\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval() # Set the model to evaluation mode\n",
    "correct = 0  # Initialize the number of correctly predicted samples\n",
    "total = 0  # Initialize the total number of samples\n",
    "\n",
    "with torch.no_grad(): # Use torch.no_grad() to disable gradient computation during testing\n",
    "    for images, labels in test_loader: # Iterate through batches of test data\n",
    "        images, labels = images.to(device), labels.to(device) # Move data to GPU if available\n",
    "        outputs = model(images) # Forward pass through the model\n",
    "        _, predicted = torch.max(outputs.data, 1) # Get the predicted class labels\n",
    "        total += labels.size(0) # Increment total number of images\n",
    "        correct += (predicted == labels).sum().item() # Count correctly predicted images\n",
    "\n",
    "accuracy = correct / total # Calculate accuracy\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\") # Print the accuracy on the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRY 3: USING PRETRAINED MODEL: RESNET 18 FOR ACHIEVING BETTER ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1, Loss: 1.3403009552784892\n",
      "Epoch 2, Loss: 1.0638465591708717\n",
      "Epoch 3, Loss: 0.9708040430570197\n",
      "Epoch 4, Loss: 0.9199596481859836\n",
      "Epoch 5, Loss: 0.8820667493983608\n",
      "Epoch 6, Loss: 0.8590083898943098\n",
      "Epoch 7, Loss: 0.8284356531203555\n",
      "Epoch 8, Loss: 0.8084617858500127\n",
      "Epoch 9, Loss: 0.788709190228711\n",
      "Epoch 10, Loss: 0.7772868377016023\n",
      "Accuracy on the test set: 72.46%\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "# This includes the essential PyTorch and torchvision modules for working with neural networks and datasets.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define the transform for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.RandomResizedCrop(32),    # Randomly crop the image and resize to 32x32\n",
    "    transforms.ToTensor(),               # Convert image to PyTorch tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image pixel values\n",
    "])\n",
    "\n",
    "# Download and load the CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) # Create training dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) # Create test dataset\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Use a pre-trained ResNet18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)  # Change the output layer to have 10 classes\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Check if GPU is available, else use CPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # SGD optimizer with momentum\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Set the number of training epochs\n",
    "# Iterate over each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0  # Variable to track the running loss during each epoch\n",
    "    # Iterate over the training dataset\n",
    "    for i, data in enumerate(trainloader, 0): \n",
    "        inputs, labels = data # Get inputs and labels for the current batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # Move data to GPU if available\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate the loss\n",
    "        loss.backward()  # Backward pass to compute gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "        running_loss += loss.item()  # Accumulate the running loss\n",
    "\n",
    "    # Print the average loss for the current epoch\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}\")\n",
    "\n",
    "\n",
    "# Testing the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0  # Variable to track the number of correctly predicted images\n",
    "total = 0  # Variable to track the total number of images in the test set\n",
    "\n",
    "with torch.no_grad(): # Use torch.no_grad() to disable gradient computation during testing\n",
    "    for data in testloader:  # Iterate over batches in the test loader\n",
    "        images, labels = data # Get inputs and labels for the current batch\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU if available\n",
    "        outputs = model(images)  # Forward pass\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predicted class labels\n",
    "        total += labels.size(0)  # Increment total number of images\n",
    "        correct += (predicted == labels).sum().item()  # Count correctly predicted images\n",
    "\n",
    "accuracy = 100 * correct / total # Calculate accuracy\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}%\") # Print the test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRY 4: USING ANOTHER PRETRAINED MODEL: DENSENET 121 MODEL FOR BETTER ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\divig\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\divig\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to C:\\Users\\divig/.cache\\torch\\hub\\checkpoints\\densenet121-a639ec97.pth\n",
      "100%|██████████| 30.8M/30.8M [00:08<00:00, 3.61MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7561923265457153\n",
      "Epoch 2/10, Loss: 0.17223471403121948\n",
      "Epoch 3/10, Loss: 0.3745187222957611\n",
      "Epoch 4/10, Loss: 0.5094252824783325\n",
      "Epoch 5/10, Loss: 0.16775503754615784\n",
      "Epoch 6/10, Loss: 0.2698609232902527\n",
      "Epoch 7/10, Loss: 0.1625276356935501\n",
      "Epoch 8/10, Loss: 0.5435947179794312\n",
      "Epoch 9/10, Loss: 0.18278992176055908\n",
      "Epoch 10/10, Loss: 0.5831239223480225\n",
      "Test Accuracy: 84.48%\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "# Step 1: Set device and hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check if GPU is available, else use CPU\n",
    "batch_size = 64  # Number of images in each mini-batch\n",
    "learning_rate = 0.001  # Learning rate for the optimizer\n",
    "epochs = 10  # Number of times to iterate through the entire dataset during training\n",
    "\n",
    "# Step 2: Load and preprocess the CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image pixel values\n",
    "])\n",
    "\n",
    "# Download and create training and test datasets\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) # Create training dataset\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) # Create test dataset\n",
    "\n",
    "# Create DataLoader instances to efficiently load and iterate over batches of data\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Step 3: Initialize the pre-trained DenseNet model\n",
    "model = models.densenet121(pretrained=True)  # Load pre-trained DenseNet model\n",
    "# Modify the classifier for CIFAR-10 (10 classes)\n",
    "model.classifier = nn.Linear(1024, 10)  # Change the output layer to have 10 classes for CIFAR-10\n",
    "model = model.to(device)  # Move the model to the GPU (if available)\n",
    "\n",
    "# Step 4: Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer with specified learning rate\n",
    "\n",
    "# Step 5: Train the model\n",
    "for epoch in range(epochs): # Loop over the dataset for a specified number of epochs\n",
    "    model.train()  # Set the model to training mode\n",
    "    for images, labels in train_loader: # Iterate through batches of training data\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU if available\n",
    "\n",
    "        optimizer.zero_grad()  # Zero gradients to clear previous gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass to compute gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")  # Print loss after each epoch\n",
    "\n",
    "# Step 6: Evaluate the model on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0  # Initialize the number of correctly predicted samples\n",
    "total = 0  # Initialize the total number of samples\n",
    "\n",
    "# Use torch.no_grad() to disable gradient computation during testing\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader: # Iterate over batches in the test loader\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU if available\n",
    "        outputs = model(images)  # Forward pass\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predicted class labels\n",
    "        total += labels.size(0)  # Increment total number of images\n",
    "        correct += (predicted == labels).sum().item()  # Count correctly predicted images\n",
    "\n",
    "accuracy = correct / total  # Calculate accuracy\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")  # Print test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IN CONCLUSION,I FOUND OUT THAT THROUGH THE PRETRAINED MODEL DENSENET121 I ACHIEVED THE BEST ACCURACY AMONG ALL THE OTHER WAYS WHICH IS EQUAL TO 84.48% ACCURACY WHICH IS QUITE IMPRESSIVE "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
